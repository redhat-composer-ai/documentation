{"Infrastructure/Cluster-Infrastructure":{"title":"Openshift Cluster Infrastructure","links":[],"tags":["infra","openshift","cluster","infrastructure"],"content":"Openshift Cluster Infrastructure\n\n\n                  \n                  Note\n                  \n                \nCurrently most of our testing is done using an Openshift ROSA Cluster. Minor adjustments may be required if using another cluster.\n\nQuickstart Guide\nThe easiest way to install Composer on a new cluster is using the bootstrap script in the Cluster Gitops repository. This script automates the following:\n\nInstalls all required operators\nProvisions GPU nodes\nDeploys the Composer application\n\nFor detailed information, refer to the repository’s README.\nRequired Operators\nIf you are using an existing cluster or performing a custom installation, ensure the following operators are installed:\n\nOpenShift AI\nOpenShift Service Mesh\nOpenShift Serverless\nAuthorino\nNVIDIA GPU Operator\nNode Feature Discovery\nOpenShift GitOps\nOpenShift Pipelines\nElasticsearch (or another supported vector database)\n\n\n\n                  \n                  Note\n                  \n                \nElasticsearch is the default vector database for demonstration purposes.\n\nWhen using the bootstrap script, these operators are automatically installed in the openshift-gitops namespace by the OpenShift GitOps operator.\nDefault Composer Installation\nThe bootstrap script also creates two namespaces:\n\ncomposer-ai-gitops: Hosts an Argo CD instance that manages the Composer application.\ncomposer-ai-apps: The namespace where the Composer application resides.\n\nThe Argo CD instance deploys Composer using the Helm chart located in the App of Apps repository.\nFor custom installations on existing clusters, it is recommended to install this Helm chart using GitOps for streamlined management. Refer to the Composer AI Infrastructure documentation for detailed instructions."},"Infrastructure/Composer-AI":{"title":"Composer AI Infrastructure","links":["Infrastructure/Cluster-Infrastructure"],"tags":["infra"],"content":"Composer AI Infrastructure\nOverview\n\nThe current deployed applications include the following Composer components:\n\n\nChat UI with Composer Studio - A PatternFly based web UI designed to allow users to easily create new chat assistants and interact with existing assistants.\n\n\nConductor Microservice - A Quarkus based API that hosts the various assistants, which can leverage a RAG pattern with a vector database and various LLMs, and acts as a gateway for any chat application.\n\n\nDocument Ingestion Pipeline - A pipeline that allows users to ingest documents into a vector database using both Tekton and Data Science Pipelines\n\n\nInstall\nPrerequisite\nCluster Setup contains all the required infrastructure\n\n\n                  \n                  Tip\n                  \n                \nIf the Cluster GitOps repo was used for cluster setup, then the Composer AI application should be installed by default\n\nDeploy Components\nInstalling the components directly can be done from the App Of Apps Repository. The install is done through a helm chart located in appOfApps, it has been tested deploying through ArgoCD."},"Infrastructure/Overview":{"title":"Infrastructure Overview","links":["Infrastructure/Cluster-Infrastructure","Infrastructure/Composer-AI"],"tags":["infra","infrastructure"],"content":"Architecture\nThe Composer AI Project includes both the code base and a working tech stack for setting up the base generative AI solution, including:\n\nComposer AI Frontend Application (PatternFly)\nConductor (Quarkus API Server)\nLLM Serving Runtime (Granite installed on vLLM)\nVector Database (Elastic)\nExample Ingestion Pipeline (Tekton/KFP)\n\nThis allows for developers to be able to quickly deploy the application. Note if you have access to the Red Hat demo platform, there is a pre-provisioned demo that creates a fully functional instance of the application on demand.\nInstall Process\n\nSetup Cluster Infrastructure\nInstall Composer AI Application\n"},"Ingestion/Debug-Guide":{"title":"Ingestion Debug Guide","links":[],"tags":["ingestion"],"content":"Introduction\n"},"Ingestion/Overview":{"title":"Ingestion Overview","links":[],"tags":["ingestion","debug","faq"],"content":"Overview\nData Ingestion is done through a DataSciencePipeline\n\nValidation\nRuns of the pipeline can be seen through the Data Science UI by navigating to Experiments -&gt; Experiments and Runs -&gt; documentation_ingestion"},"User-Guide/Chatbot-UI":{"title":"Chatbot UI","links":[],"tags":["user_guide","end_user","chatbot-ui","chatbot","user-guide"],"content":"Introduction\nComposer AI is broken down into three main parts\n"},"User-Guide/Conductor-API":{"title":"Conductor API","links":[],"tags":["user_guide","end_user","api","conductor","chatbot","user-guide"],"content":"Introduction\nConductor API is the API that is used when interacting with\n"},"User-Guide/Overview":{"title":"Overview","links":[],"tags":["user_guide","end_user"],"content":"Introduction\nComposer AI is broken down into three main parts\n"},"demo/Developer-Guide":{"title":"Demo Developer Guide","links":[],"tags":["Demo","Developer-Guide"],"content":"Developer Demo Environment\nThe default Composer AI Demo Environment is pointed to the latest stable tag in the Composer AI Repository.\nChange Gitops Tag\nIt may be needed to point to a different tag, branch or even repo when developing content for Composer AI.\nTurning of AutoSync\nBecause most of Composer runs on App of Apps or the ApplicationSet pattern it can be difficult to turn of autosync. The following instructions will turn off the top level auto sync for most components.\n\nDisable auto sync for Application bootstrap in the main GitOps: oc patch application bootstrap --type=&#039;merge&#039; -p &#039;{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;syncPolicy&quot;: {&quot;automated&quot;: {&quot;selfHeal&quot;: false}}}}}}&#039; -n openshift-gitops\nDisable auto sync for ApplicationSet tenants on the openshift-gitops namespace: oc patch applicationset tenants --type=&#039;merge&#039; -p &#039;{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;syncPolicy&quot;: {&quot;automated&quot;: {&quot;selfHeal&quot;: false}}}}}}&#039; -n openshift-gitops\nDisable auto sync for Application composer-ai-config in the Composer AI GitOps: oc patch application bootstrap --type=&#039;merge&#039; -p &#039;{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;syncPolicy&quot;: {&quot;automated&quot;: {&quot;selfHeal&quot;: false}}}}}}&#039; -n composer-ai-gitops\n\nAt this most of the other Composer AI applications should be modifyable without being controlled by a parent Argo App.\n\n\n                  \n                  Tip\n                  \n                \nThere are a some Application in the Cluster GitOps that may still need extra work in order to prevent auto sync.\n"},"demo/Run-a-Composer-AI-local-env":{"title":"Run a Composer AI local env","links":[],"tags":[],"content":"Clone the front and back end git repos:\ngithub.com/redhat-composer-ai/quarkus-llm-router\ngithub.com/redhat-composer-ai/chatbot-ui\nPrerequisites for Mac users\nInstall Podman\nDownload and install from podman.io/\n\n\n                  \n                  NOTE\n                  \n                \nFor Mac users:\nRun podman machine init\nRun podman machine start\n\nInstall maven\nMac:\nbrew install maven\nLinux:\ndnf install maven\nInstall java version 21\nwww.oracle.com/java/technologies/downloads/#java21\n\n\n                  \n                  NOTE\n                  \n                \nMac Users:\nIf you have updated the apple version of java, and it is at a version higher than java 21, you may need to set the JAVA_HOME environment variable to that maven picks up the correct version:\nedit .zshenv\nadd this line to the file:\nexport JAVA_HOME=$(/usr/libexec/java_home)\nsource the new setting:\nsource .zshenv\n\nCheck the versions of maven and java:\njava --version\nmvn --version\nInstall npm\nMac:\nbrew install npm\nLinux:\ndnf install npm\nInstall sirv\nnpm install --save sirv\nInstall LLM Router\nCreate application-local.properties file (example below) and copy into src/main/resources of the quarkus-llm-router directory. You will need to fill in the LLM information in the first three lines (github.com/redhat-composer-ai/quarkus-llm-router#llm-connection)\nopenai.default.url=mistral-7b-instruct-v0-3-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\nopenai.default.apiKey=***********\nopenai.default.modelName=mistral-7b-instruct\n\nelasticsearch.default.apiKey=nothing\nelasticsearch.default.host=localhost\nelasticsearch.default.index=test_index\n\n\ndisable.authorization=true\n\nquarkus.http.cors=true\nquarkus.http.cors.origins=*\nquarkus.http.cors.origins=http://localhost:9000\nquarkus.http.cors.methods=GET,PUT,POST,DELETE,PATCH,OPTIONS\nquarkus.http.cors.headers=accept,authorization,content-type,x-requested-with\nquarkus.http.cors.exposed-headers=location,info\nquarkus.http.cors.access-control-max-age=24H\nquarkus.http.cors.allow-credentials=true\n\n# Set liquibase context (note if using multiple quarkus profiles last one takes precedence)\nquarkus.liquibase-mongodb.contexts=local\n\nMac users: make sure your podman machine has started per the instructions above.\nRun mvn clean install in the quarkus-llm-router directory.\nRun mvn quarkus:dev -Dquarkus.profile=local in the quarkus-llm-router directory.\nChatbot-UI Install\nIn a new terminal, cd into the chatbot-ui directory (from github.com/redhat-composer-ai/chatbot-ui)\nrun npm install\nrun npm run openapi\nrun npm run start:dev\nThis should open a browser window with the Chatbot-UI:\n\nMake sure to choose an Assistant:\n\nClick on LLM Connections to make sure you have a default connection:\n\nAsk your LLM a question:\n"},"demo/Setup":{"title":"Demo Environment Setup","links":["demo/walkthrough"],"tags":["Demo"],"content":"Red Hat Demo Environment\nComposer AI is available as a catalog item for Red Hat employees and partners at demo.redhat.com:\nComposer AI Demo Environment\nAfter requesting and provisioning the demo cluster, the environment will take some time to finish setting up in order to make all of the Assistants available.\nPlease refer to the Environment Validation Instructions in the Demo Walkthrough to verify the completion of the automated setup."},"demo/walkthrough":{"title":"Demo Environment Walkthrough","links":["User-Guide/Chatbot-UI","User-Guide/Conductor-API","Ingestion/Overview"],"tags":["Demo"],"content":"Overview\nThis is a demo for deploying Composer AI.\nComposer AI is Red Hat’s cutting-edge platform for rapidly developing and deploying generative AI solutions that transform enterprise workflows. Designed to integrate seamlessly with Red Hat OpenShift, Composer AI enables users to create AI assistants tailored to specific business needs, leveraging technologies like LangChain4J, Vector Databases, and GraphRAG. These assistants enhance productivity by automating tasks such as summarization, data retrieval, project management, and decision support. Composer AI simplifies the deployment process while providing robust support for hybrid cloud environments, and a focus on scalability and security, Composer AI empowers organizations to streamline operations, solve complex problems, and innovate at scale—all while leveraging Red Hat’s trusted infrastructure.\nThis environment includes the following components for composer:\n\nThe Composer UI - Pattern Fly Frontend\nThe Conductor API - Quarkus/Langchain4J API\nLLM Instance - Granite modes running on vLLM Serving Runtime\nVector Database - Elastic Search DB\nIngestion Pipelines - Series of KFP Pipelines ingesting Red Hat Documentation\n\nEnvironment Information\nAll of the components relevant to Composer AI are located in the composer-ai-apps namespace, and can be viewed inside the Develolper -&gt; Topology view\n\nThe Cluster Level and Composer Level Gitops used to deploy the application can be accessed through the “waffle” button on the top right.\nAnd the endpoint for the UI can be found on either the Topology view or Networking -&gt; Routes\nEnvironment Validation Instructions\nEstimated Setup Time: Approximately 30 minutes\nBefore you begin: Please allow up to 30 minutes after the cluster becomes available for the Composer AI demo environment to be fully operational. The UI and API will be available sooner, but the assistants will not be ready until ingestion is complete.\n\n\n                  \n                  Warning\n                  \n                \nPlease read the following section to validate the environment has come up before reporting issues.\n\nLLM Validation\n\nNavigate to the composer-ai-namespace, Serverless -&gt; Serving.\nVerify the Knative service is running (see images below).\n\nSuccess:\n\nFailed/Pending\n\n\n\n                  \n                  Tip\n                  \n                \nIf the LLM is down, check Conditions section under the Details tab of the Knative Service for more debugging information\n\n\nOnce the LLM is running, the Default/Meeting Minute/Email Summary/Email Draft Assistants should be functional. Other assistants will be available after ingestion is complete\n\nIngestion Validation\n\n\nThe ingestion pipeline usually takes about 30 minutes to complete after the cluster is up.\n\n\nA job on the cluster will wait for all the required resources to be created and then kick off the ingestion-pipeline Tekton Pipeline.\n\n\nMonitor the ingestion-pipeline by navigating to Pipelines -&gt; Pipelines\n\n\nOnce complete this will kick of a KFP pipeline\n\n\nTo track ingestion progress:\n\nOpen the Red Hat Openshift AI UI (waffle button at the top).\nGo to Experiments -&gt; Experiments and runs -&gt; document_ingestion.\nEnsure all tasks have completed successfully. \n\n\n\n\nTip: If the pipeline encounters a connection issue, restart it from the Pipelines view on Openshift using the Start Last Run option.\n"},"index":{"title":"Overview","links":[],"tags":["infra"],"content":"Introduction\nRed Hat Composer AI is a full-stack deployment of resources needed to launch generative AI solutions. It streamlines the entire AI development lifecycle, from concept to deployment, by leveraging the security and reliability of Red Hat OpenShift and Red Hat Openshift AI technologies. This allows enterprises to secure their data and customize their infrastructure while rapidly designing, developing, and releasing innovative generative AI solutions. Composer AI empowers developers to accelerate AI adoption with confidence, knowing their solutions are built on a secure and trusted foundation.\nKey Goals\n\nDemocratize AI: Empower business leaders, analysts, and operations teams with tools for creating generative AI solutions without requiring deep technical knowledge\nStreamline AI Management: Establish a central hub for AI solutions, promoting transparency and efficiency.\nPrioritize Data Security: Ensure all data remains securely within the cluster\nFull Stack Deployment: Provide a comprehensive, full-stack installation with all necessary components.\n"}}